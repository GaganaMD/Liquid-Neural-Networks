{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n"
      ],
      "metadata": {
        "id": "a_hNbH0d8QD7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ut175j6r0c57"
      },
      "outputs": [],
      "source": [
        "# Define a single liquid neuron that updates its state based on inputs and previous state\n",
        "class LiquidNeuron(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        \"\"\"\n",
        "        Initialize the LiquidNeuron with input and recurrent weights.\n",
        "        Args:\n",
        "        - input_dim: Dimension of input features\n",
        "        - hidden_dim: Dimension of the hidden state\n",
        "        \"\"\"\n",
        "        super(LiquidNeuron, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Parameters for input-to-hidden connections\n",
        "        self.weight_input = nn.Parameter(torch.randn(input_dim, hidden_dim))\n",
        "\n",
        "        # Parameters for hidden-to-hidden (recurrent) connections\n",
        "        self.weight_recurrent = nn.Parameter(torch.randn(hidden_dim, hidden_dim))\n",
        "\n",
        "        # Bias term for the neuron\n",
        "        self.bias = nn.Parameter(torch.zeros(hidden_dim))\n",
        "\n",
        "    def forward(self, x, state):\n",
        "        \"\"\"\n",
        "        Perform a forward pass for a single time step.\n",
        "        Args:\n",
        "        - x: Input at the current time step (batch_size x input_dim)\n",
        "        - state: Previous hidden state (batch_size x hidden_dim)\n",
        "\n",
        "        Returns:\n",
        "        - new_state: Updated hidden state\n",
        "        \"\"\"\n",
        "        # Compute the state update using a simplified differential equation\n",
        "        state_update = torch.tanh(x @ self.weight_input + state @ self.weight_recurrent + self.bias)\n",
        "\n",
        "        # Update the hidden state (simplified Euler integration)\n",
        "        new_state = state + state_update\n",
        "\n",
        "        return new_state\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Liquid Neural Network model\n",
        "class LiquidNeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        \"\"\"\n",
        "        Initialize the Liquid Neural Network.\n",
        "        Args:\n",
        "        - input_dim: Dimension of input features\n",
        "        - hidden_dim: Dimension of the hidden state\n",
        "        - output_dim: Dimension of the output\n",
        "        \"\"\"\n",
        "        super(LiquidNeuralNetwork, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # The recurrent core consisting of liquid neurons\n",
        "        self.liquid_neuron = LiquidNeuron(input_dim, hidden_dim)\n",
        "\n",
        "        # Fully connected layer to map hidden state to output\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Perform a forward pass through the network.\n",
        "        Args:\n",
        "        - x: Input tensor (batch_size x seq_len x input_dim)\n",
        "\n",
        "        Returns:\n",
        "        - output: Final output of the network (batch_size x output_dim)\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, input_dim = x.size()\n",
        "\n",
        "        # Initialize the hidden state as zeros\n",
        "        state = torch.zeros(batch_size, self.hidden_dim)\n",
        "\n",
        "        # Process each time step in the sequence\n",
        "        for t in range(seq_len):\n",
        "            state = self.liquid_neuron(x[:, t, :], state)  # Update the state with each time step\n",
        "\n",
        "        # Pass the final hidden state through the fully connected layer\n",
        "        output = self.fc(state)\n",
        "        return output\n",
        "\n"
      ],
      "metadata": {
        "id": "szgdEpY20iEk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Set input, hidden, and output dimensions\n",
        "    input_dim = 10  # Number of input features (10-dimensional input at each timestep)\n",
        "    hidden_dim = 20  # Size of the hidden layer (internal neuron state)\n",
        "    output_dim = 1  # Size of the output layer (single prediction for each sequence)\n",
        "\n",
        "    # Instantiate the Liquid Neural Network model\n",
        "    model = LiquidNeuralNetwork(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "    # Define the loss function (Mean Squared Error for regression)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Define the optimizer (Adam optimizer for efficient gradient updates)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    # Dummy training data (batch_size=32, sequence_length=5, input_dim=10)\n",
        "    inputs = torch.randn(32, 5, input_dim)  # 32 sequences, each of length 5 with 10 features per time step\n",
        "    targets = torch.randn(32, output_dim)  # Corresponding target values (one target per sequence)\n",
        "\n",
        "    # Print the head (first few rows) of the input data and target\n",
        "    print(\"Head of the Input Data (first sequence in the batch):\")\n",
        "    print(inputs[0])  # Print the first sequence in the batch (size: [5, 10])\n",
        "\n",
        "    print(\"\\nCorresponding Target (Expected output):\")\n",
        "    print(targets[0])  # Print the target for the first sequence\n",
        "\n",
        "    # Training loop (only for a few epochs to demonstrate)\n",
        "    for epoch in range(5):  # Limiting to 5 epochs for demonstration\n",
        "        optimizer.zero_grad()  # Zero the gradients from the previous step\n",
        "\n",
        "        # Forward pass: Compute model outputs\n",
        "        outputs = model(inputs)  # Pass the input data through the network\n",
        "\n",
        "        # Print the outputs (predictions of the model)\n",
        "        print(f\"\\nEpoch {epoch+1} - Model's Output for the first sequence:\")\n",
        "        print(outputs[0])  # Print the model's prediction for the first sequence\n",
        "\n",
        "        # Compute the loss between predicted outputs and target values\n",
        "        loss = criterion(outputs, targets)  # MSE Loss (Mean Squared Error)\n",
        "\n",
        "        # Backward pass: Compute gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Update model parameters using the optimizer\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print the loss after each epoch\n",
        "        print(f\"Loss after epoch {epoch+1}: {loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "9OPfwKeV0qxf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35e3d3a6-8b9f-4554-da2d-3cd6c4cebb53"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Head of the Input Data (first sequence in the batch):\n",
            "tensor([[ 0.3266,  1.0728,  1.5481, -0.8662, -0.1960, -0.5100,  0.2629, -0.0767,\n",
            "         -1.8771,  0.2020],\n",
            "        [-0.3393, -1.2468, -0.1941,  0.5752,  0.6814, -0.0170,  0.4620,  0.5776,\n",
            "          1.5289, -2.5243],\n",
            "        [-0.4314, -0.1533, -0.6434, -1.8260, -0.5662, -2.0817, -1.1770,  1.5435,\n",
            "          0.1898, -0.2435],\n",
            "        [ 0.7885,  1.7689, -0.2101, -0.1192,  0.4306,  1.0471, -1.6873, -0.0904,\n",
            "          0.1646, -1.5400],\n",
            "        [ 2.1102,  0.3155,  0.3826, -1.3880,  0.7542, -0.4804,  0.1008,  0.4054,\n",
            "          1.9530, -0.2530]])\n",
            "\n",
            "Corresponding Target (Expected output):\n",
            "tensor([-2.3060])\n",
            "\n",
            "Epoch 1 - Model's Output for the first sequence:\n",
            "tensor([0.4523], grad_fn=<SelectBackward0>)\n",
            "Loss after epoch 1: 5.9513\n",
            "\n",
            "Epoch 2 - Model's Output for the first sequence:\n",
            "tensor([0.4460], grad_fn=<SelectBackward0>)\n",
            "Loss after epoch 2: 5.7352\n",
            "\n",
            "Epoch 3 - Model's Output for the first sequence:\n",
            "tensor([0.4397], grad_fn=<SelectBackward0>)\n",
            "Loss after epoch 3: 5.5278\n",
            "\n",
            "Epoch 4 - Model's Output for the first sequence:\n",
            "tensor([0.4339], grad_fn=<SelectBackward0>)\n",
            "Loss after epoch 4: 5.3255\n",
            "\n",
            "Epoch 5 - Model's Output for the first sequence:\n",
            "tensor([0.4282], grad_fn=<SelectBackward0>)\n",
            "Loss after epoch 5: 5.1160\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rvwJFFvT0tWR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}